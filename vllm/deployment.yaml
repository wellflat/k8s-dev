apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-server
  namespace: default
  labels:
    app: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      containers:
      - name: vllm
        #image: vllm/vllm-openai:v0.11.0
        image: vllm/vllm-openai:v0.10.2
        ports:
          - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 8
          requests:
            nvidia.com/gpu: 8      
        env:
        - name: MODEL_ID
          value: openai/gpt-oss-120b
          #value: google/gemma-3-27b-it
          #value: elyza/Llama-3-ELYZA-JP-8B
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token-secret
              key: hf_token
        - name: HF_HOME
          #value: /workspace
          value: /root/.cache/huggingface
        command: ["/bin/sh", "-c"]
        args:
           - |
            vllm serve ${MODEL_ID} --config /etc/vllm/config.yaml
        volumeMounts:
          - name: hf-data
            mountPath: /root/.cache/huggingface/hub
            #mountPath: /workspace/hub
          - name: shm
            mountPath: /dev/shm
          - name: config-volume
            mountPath: /etc/vllm
            readOnly: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 60
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 60
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 240
      volumes:
        - name: hf-data
          persistentVolumeClaim:
            claimName: hf-data-pvc
        # vLLM needs to access the host's shared memory for tensor parallel inference.
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "20Gi"
        - name: config-volume
          configMap:
            name: vllm-config
            items:
              - key: config.yaml
                path: config.yaml
