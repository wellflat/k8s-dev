apiVersion: batch/v1
kind: Job
metadata:
  name: genai-perf-job
  labels:
    app: genai-perf
spec:
  template:
    spec:
      containers:
        - name: genai-perf-container
          image: nvcr.io/nvidia/tritonserver:25.07-py3-sdk
          resources:
            limits:
              nvidia.com/gpu: 1
            requests:
              nvidia.com/gpu: 1
          volumeMounts:
            - name: hf-data
              mountPath: /root/.cache/huggingface/hub
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: all
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: all
            - name: MODEL_ID
              value: openai/gpt-oss-20b
            - name: ARTIFACT_BASE_DIR
              value: /root/.cache/huggingface/hub/benchmark_results/genai-perf
          command: ["/bin/bash", "-c"]
          args:
           - |
            genai-perf profile -m ${MODEL_ID} \
            --endpoint-type chat \
            --synthetic-input-tokens-mean 200 \
            --synthetic-input-tokens-stddev 30 \
            --output-tokens-mean 300 \
            --output-tokens-stddev 30 \
            --num-prompts 200  \
            --concurrency 1 \
            --random-seed 42 \
            --streaming \
            --request-count 200 \
            --warmup-request-count 10 \
            --url http://vllm-service:80 \
            --server-metrics-url http://dcgm-exporter.prometheus.svc.cluster.local:9400/metrics \
            --artifact-dir ${ARTIFACT_BASE_DIR}/${MODEL_ID}/ \
            --verbose
      restartPolicy: Never
      volumes:
        # This volume mount is kept from the original template in case you need to
        # interact with other cached models within this step.
        - name: hf-data
          persistentVolumeClaim:
            claimName: hf-data-pvc
      nodeSelector:
        nvidia.com/gpu.product: "NVIDIA-A100-SXM4-80GB"
