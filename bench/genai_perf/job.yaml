apiVersion: batch/v1
kind: Job
metadata:
  name: genai-perf-job
  labels:
    app: genai-perf
spec:
  template:
    spec:
      containers:
        - name: genai-perf-container
          image: nvcr.io/nvidia/tritonserver:25.01-py3-sdk
          resources:
            limits:
              nvidia.com/gpu: 1
            requests:
              nvidia.com/gpu: 1
          volumeMounts:
            - name: llm-bench-model
              mountPath: /root/.cache/huggingface/hub
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: all
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: all
            - name: MODEL_ID
              value: meta-llama/Llama-3.1-8B-Instruct
              #value: elyza/Llama-3-ELYZA-JP-8B
          command: ["/bin/bash", "-c"]
          args:
           - |
            genai-perf profile -m ${MODEL_ID} \
            --service-kind openai \
            --endpoint-type chat \
            --synthetic-input-tokens-mean 150 \
            --synthetic-input-tokens-stddev 30 \
            --output-tokens-mean 300 \
            --output-tokens-stddev 30 \
            --num-prompts 300  \
            --concurrency 1 \
            --measurement-interval 1800000 \
            --random-seed 42 \
            --streaming \
            --url http://vllm-service:80 \
            --artifact-dir /root/.cache/huggingface/hub/benchmark_results/genai-perf/${MODEL_ID}/vllm \
            --generate-plots
      restartPolicy: Never
      volumes:
        - name: llm-bench-model
          persistentVolumeClaim:
            claimName: llm-bench-model-pvc

