apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-server
  namespace: default
  labels:
    app: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      containers:
      - name: vllm
        #image: vllm/vllm-openai:v0.11.0
        image: vllm/vllm-openai:v0.10.2
        ports:
          - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 4
          requests:
            nvidia.com/gpu: 4
        env:
        - name: MODEL_ID
          value: openai/gpt-oss-120b
          #value: google/gemma-3-27b-it
          #value: elyza/Llama-3-ELYZA-JP-8B
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token-secret
              key: hf_token
        - name: HF_HOME
          #value: /workspace
          value: /root/.cache/huggingface
        command: ["/bin/sh", "-c"]
        args:
           - |
            vllm serve ${MODEL_ID} \
            --dtype=auto \
            --trust-remote-code \
            --enable-chunked-prefill \
            --enforce-eager \
            --swap-space=8 \
            --max-model-len=16384 \
            --max-num-batched-tokens=8192 \
            --gpu-memory-utilization=0.85 \
            --tensor-parallel-size=4 \
            --async-scheduling
        #--quantization fp8
        #Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
        #securityContext:
        #  runAsUser: 1000
        volumeMounts:
          - name: hf-data
            mountPath: /root/.cache/huggingface/hub
            #mountPath: /workspace/hub
          - name: shm
            mountPath: /dev/shm
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 60
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 60
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 240
      volumes:
        - name: hf-data
          persistentVolumeClaim:
            claimName: hf-data-pvc
        # vLLM needs to access the host's shared memory for tensor parallel inference.
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "20Gi"
