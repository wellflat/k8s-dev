GROUP:      kubeai.org
KIND:       Model
VERSION:    v1

FIELD: spec <Object>


DESCRIPTION:
    ModelSpec defines the desired state of Model.
    
FIELDS:
  adapters	<[]Object>
    <no description>

  args	<[]string>
    Args to be added to the server process.

  autoscalingDisabled	<boolean>
    AutoscalingDisabled will stop the controller from managing the replicas
    for the Model. When disabled, metrics will not be collected on server Pods.

  cacheProfile	<string>
    CacheProfile to be used for caching model artifacts.
    Must be a valid CacheProfile defined in the system config.

  engine	<string> -required-
  enum: OLlama, VLLM, FasterWhisper, Infinity
    Engine to be used for the server process.

  env	<map[string]string>
    Env variables to be added to the server process.

  envFrom	<[]Object>
    Env variables to be added to the server process from Secret or ConfigMap.

  features	<[]string> -required-
    Features that the model supports.
    Dictates the APIs that are available for the model.

  files	<[]Object>
    Files to be mounted in the model Pods.

  image	<string>
    Image to be used for the server process.
    Will be set from ResourceProfile + Engine if not specified.

  loadBalancing	<Object>
    LoadBalancing configuration for the model.
    If not specified, a default is used based on the engine and request.

  maxReplicas	<integer>
    MaxReplicas is the maximum number of Pod replicas that the model can scale
    up to.
    Empty value means no limit.

  minReplicas	<integer>
    MinReplicas is the minimum number of Pod replicas that the model can scale
    down to.
    Note: 0 is a valid value.

  owner	<string>
    Owner of the model. Used solely to populate the owner field in the
    OpenAI /v1/models endpoint.
    DEPRECATED.

  priorityClassName	<string>
    PriorityClassName sets the priority class for all pods created for this
    model.
    If specified, the PriorityClass must exist before the model is created.
    This is useful for implementing priority and preemption for models.

  replicas	<integer>
    Replicas is the number of Pod replicas that should be actively
    serving the model. KubeAI will manage this field unless AutoscalingDisabled
    is set to true.

  resourceProfile	<string>
    ResourceProfile required to serve the model.
    Use the format "<resource-profile-name>:<count>".
    Example: "nvidia-gpu-l4:2" - 2x NVIDIA L4 GPUs.
    Must be a valid ResourceProfile defined in the system config.

  scaleDownDelaySeconds	<integer> -required-
    ScaleDownDelay is the minimum time before a deployment is scaled down after
    the autoscaling algorithm determines that it should be scaled down.

  targetRequests	<integer> -required-
    TargetRequests is average number of active requests that the autoscaler
    will try to maintain on model server Pods.

  url	<string> -required-
    URL of the model to be served.
    Currently the following formats are supported:
    
    
    For VLLM, FasterWhisper, Infinity engines:
    
    
    "hf://<repo>/<model>"
    "pvc://<pvcName>"
    "pvc://<pvcName>/<pvcSubpath>"
    "gs://<bucket>/<path>" (only with cacheProfile)
    "oss://<bucket>/<path>" (only with cacheProfile)
    "s3://<bucket>/<path>" (only with cacheProfile)
    
    
    For OLlama engine:
    
    
    "ollama://<model>"


